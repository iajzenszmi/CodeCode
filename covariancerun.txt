ian@ian-HP-Stream-Laptop-11-y0XX:~$ ls
 aima-python
 a.out
 arct.txt
 atanf
 atanf.f
 atlasf
 auto_examples_python.zip
 banmul.f
 bergenoceanforecast
 cabinetsort.py
 chebc
 chebc.c
 chebcpp.cpp
 chebcsourcecodeandrunlisting.txt
 chebfun.jl
 chebjul.jul
 CodeCode
 data.txt
 Desktop
 Documents
 Downloads
 euclid.py
 F77
 F77.1
 F77.2
 fckit
'Game Graphics Programming By Allen Sherrod.pdf.1'
 GB8ZMQZ3
 gitkey.txt
 gitkey.txt.pub
 gradient
 gradient2
 gradient2.f
 gradient2runlisting.txt
 gradient.f
 graphtest
 graphtest.f08
 handbookartificialintelligence
 hellow
 hellow.f
 index.html
 index.html.1
'index.html?extid=SEO----'
 iris
 Music
 nano.save
 nohup.out
 parab
 parabola
 parabola.f
 para.txt
 Pictures
 pli-0.9.10d
 pli-0.9.10d.tgz
 plot2.plt
 plot3.plt
 plot.plt
 Public
'Rabbi YY Jacobson - The Satmar Rebbe on Struggling Children-JXhlkrDm2U0.f248.webm.part'
 randa.py
 sgsol.f
 sin
 sine
 sine.f
 sin.f
 sin.plt
 sin.txt
 sklearnexamples
 snap
 spawnchildibm
 spawnchildibm.c
 spawnchildibmrunlisting.txt
 spawnparentibm.c
 spawntest.c.save
 spawntestibm.c
 spawntest.out
 synchp
 synchprims.cpp
 SzeliskiBookDraft_20210702.pdf.1
 temperature.py
 templat2.py
 template.py
 Templates
 testclisting.txt
 testf
 testf.f
 testf.o
 testfun.c
 test.m
 tinvb.f
 tmp
 Videos
 waterbalance
 wget-log
 www.patarnott.com
 xbanmul
 xbanmul.f
 xsplinerunwithgnuplot.txt
ian@ian-HP-Stream-Laptop-11-y0XX:~$ cd sklearnexamples
ian@ian-HP-Stream-Laptop-11-y0XX:~/sklearnexamples$ ls
applications               decomposition         mixture
auto_examples_jupyter.zip  ensemble              model_selection
auto_examples_python.zip   exercises             multioutput
bicluster                  feature_selection     neighbors
calibration                gaussian_process      neural_networks
classification             impute                preprocessing
cluster                    inspection            release_highlights
compose                    kernel_approximation  semi_supervised
covariance                 linear_model          svm
cross_decomposition        manifold              text
datasets                   miscellaneous         tree
ian@ian-HP-Stream-Laptop-11-y0XX:~/sklearnexamples$ cd applications
ian@ian-HP-Stream-Laptop-11-y0XX:~/sklearnexamples/applications$ ls
plot_face_recognition.ipynb
plot_face_recognition.py
plot_model_complexity_influence.ipynb
plot_model_complexity_influence.py
plot_outlier_detection_wine.ipynb
plot_outlier_detection_wine.py
plot_out_of_core_classification.ipynb
plot_out_of_core_classification.py
plot_prediction_latency.ipynb
plot_prediction_latency.py
plot_species_distribution_modeling.ipynb
plot_species_distribution_modeling.py
plot_stock_market.ipynb
plot_stock_market.py
plot_tomography_l1_reconstruction.ipynb
plot_tomography_l1_reconstruction.py
plot_topics_extraction_with_nmf_lda.ipynb
plot_topics_extraction_with_nmf_lda.py
sklearnstockmarket.txt
svm_gui.ipynb
svm_gui.py
wikipedia_principal_eigenvector.ipynb
wikipedia_principal_eigenvector.py
ian@ian-HP-Stream-Laptop-11-y0XX:~/sklearnexamples/applications$ ipython3 plot_outlier_detection_wine.py

====================================
Outlier detection on a real data set
====================================

This example illustrates the need for robust covariance estimation
on a real data set. It is useful both for outlier detection and for
a better understanding of the data structure.

We selected two sets of two variables from the Boston housing data set
as an illustration of what kind of analysis can be done with several
outlier detection tools. For the purpose of visualization, we are working
with two-dimensional examples, but one should be aware that things are
not so trivial in high-dimension, as it will be pointed out.

In both examples below, the main result is that the empirical covariance
estimate, as a non-robust one, is highly influenced by the heterogeneous
structure of the observations. Although the robust covariance estimate is
able to focus on the main mode of the data distribution, it sticks to the
assumption that the data should be Gaussian distributed, yielding some biased
estimation of the data structure, but yet accurate to some extent.
The One-Class SVM does not assume any parametric form of the data distribution
and can therefore model the complex shape of the data much better.

First example
-------------
The first example illustrates how the Minimum Covariance Determinant
robust estimator can help concentrate on a relevant cluster when outlying
points exist. Here the empirical covariance estimation is skewed by points
outside of the main cluster. Of course, some screening tools would have pointed
out the presence of two clusters (Support Vector Machines, Gaussian Mixture
Models, univariate outlier detection, ...). But had it been a high-dimensional
example, none of these could be applied that easily.



https://scikit-learn.org/stable/auto_examples/applications/plot_outlier_detection_wine.html#sphx-glr-auto-examples-applications-plot-outlier-detection-wine-py

